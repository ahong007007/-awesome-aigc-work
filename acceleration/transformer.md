Transformer结构在大模型、CV、NLP、推荐场景里被广泛使用，已成为当前深度学习最重要的模型范式。但是，大模型的训练和推理都很烧钱，无论是OpenAI,微软，亚马逊还是国内的字节，百度，腾讯，都迫切需要大模型的加速优化。  
Transformer结构中存在(1)大量串行执行的非矩阵运算算子（2）性能较差的超越函数运算（3）访存瓶颈的超大Matmu算子，影响了Transformer大模型在昇腾芯片上的执行效率，需要软硬协同优化。  
技术价值：业界主流大模型（如GPT3/ChatGPT)都是Transformer结构，由于上述瓶颈，在千卡集群上训练时间长。训练成本高；此外，在线推理的时延性能和离线推理的吞吐性能也无法满足商业要求。急需对Transformer算子进行优化，实现大模型训练和推理性能的提升。
