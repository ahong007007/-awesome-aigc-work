**常见的模型压缩技术方案**

前提：  
• 性能要求：不变或少掉点  
• 效能期望：速度升、能耗降  
• 架构要求：不能伤筋动骨  
 
目标：  
• 模型大小：变小、大幅度变小  
• 计算效率：提升、大幅度提速  
• 软硬件适配  
• 利于系统集成  
• 可信因素变化及保障  

方案：  
• 量化压缩  
• 剪枝压缩  
• 参数共享  
• 低秩分解  
• 知识蒸馏  
• 模型缩放  

**模型压缩算法**

• 在实际应用上，针对模型，特别是针对大模型，必须经过模型压缩（轻量化）环节  
• 在业界声量上，模型训练和大模型自身的算法优化获得了极大关注，在模型压缩上被掩盖，但其重要性犹在
• 在各压缩算法中，量化、裁剪最常用，性价比高，短平快；低秩分解在大模型压缩中适用性不强；模型蒸馏的
可解释性和理论性强，除了压缩外，还经常引入到知识迁移领域，但是训练代价比较大，一般是在云端使用
• 模型量化算法中，二值量化是极限量化，具有研究意义，但是实用性不强  
• 工业界模型压缩和部署平台上，微软牵头的ONNX、 Meta的coreML、百度paddleSlim、华为mindX等，各
家各有侧重，但在实用环节，主体上采用量化压缩居多，其他算法结合，并有格式转换方面的相关合作和竞争  
• 在单模态大模型上， NLP、视觉、语音等各自独立发展， NLP的新技术涌现较多、最活跃，其次是视觉，再次
是语音，并在模型压缩上均有相关研究，在NLP大模型压缩上偏多  
• 在多模态融合大模型上，文本-视觉多模态是绝对的热点，基于AIGC、 diffussion扩散模型的文生图获得极大
关注，基于GPT3.5的chatGPT对话大模型获得极大关注， 现在这两个热点还集中在舆论关注、算法优化、积
累用户反馈的阶段，在模型压缩和部署方面还刚起步  
• 无论是普通模型压缩（有时候直接使用了，就不压缩了，具体看环境限制情况），还是大模型的压缩，在具体
压缩技术上都可以共用，基本原理和框架都是相同的，原则上都可以尝试各种量化压缩、蒸馏压缩、剪枝共享
等技术，只是在技术细节、实际应用过程中有一定差异或侧重  
• 在对模型压缩技术洞察中发现，大多数的文献主要是集中在各种压缩算法自身的研究中，或追求对极限压缩的
性能对比上，而对各种压缩方法所产生的各种可信安全问题的研究则相对较少，而这个与实际应用更紧密，因
此建议对模型压缩的可信问题方面投入更多关注  
